<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Neda Etebari" />
  <title> Udacity Capstone Project-Data Scientist Nanodegree A Machine Learning Approach to Predict a Biological Response </title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      word-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title"> Udacity Capstone Project-Data Scientist Nanodegree<br />
<span>A Machine Learning Approach to Predict a Biological Response </span></h1>
<p class="author">Neda Etebari</p>
</header>
<h1 id="definition">Definition</h1>
<h2 class="unnumbered" id="project-overview">Project Overview</h2>
<p>In pharmaceutical industry, the process of drug development is extremely expensive and time-consuming. It largely depends on trial and error in order to categorize molecules based on their biological activities. Using machine learning approaches, one can predict the biological response of a molecule only based on its physical and chemical characteristics. This task is of great importance considering the amount of time and money that could be saved with accurate predictions.</p>
<p>The field of predicting biological responses based on molecular information has been around for 45 years. Specifically, this activity is classified as quantitative structure-activity relationship (QSAR) and is used comprehensively in the fields of chemistry, pharmaceutical and toxicology. <span class="citation" data-cites="leveleux2012"></span></p>
<p>In this report, I tackle this problem using various learning models including advanced ensemble methods as well as more simple preliminary models. Results that contrast the performance of these techniques are presented.</p>
<h2 class="unnumbered" id="problem-statement">Problem Statement</h2>
<p>A new drug development is largely depends on trial and error. It requires synthesizing thousands of components that finally becomes a drug. This process is tremendously expensive and slow. If we know the biological activity of molecules (i.e. the beneficial or adverse effects of the molecules on human body) in advance, we could easily eliminate biologically inactive ones from drug development process. Hence, accurate prediction of biological response of molecules as well as understanding the rational behind those estimates would be of great importance to the pharmaceutical industries. Since, precise predictions could save thousands of dollars and so many hours of repetitive work. <span class="citation" data-cites="ajay1994"></span></p>
<p>In this project, I participated in a Kaggle competition named “predicting a biological response”, which can be categorized as a knowledge discovery application of ML in healthcare. The goal is to predict the biological activity of molecules only using their physical and chemical characteristics. In pharmacology, biological activity (response) of a molecule describes the beneficial or adverse effects of it on a living matter (e.g. human body) <span class="citation" data-cites="wikibio"></span>. The experimental process of drug development using thousands of these molecules consists of expensive and time-consuming trials and errors. By accurately predicting the biological activity of a molecule, I will have the required knowledge to decide whether to include that specific molecule in drug development process or not.</p>
<p>Various supervised and unsupervised preprocessing methods including feature selection techniques and outlier detection will be applied in order to reduce the prediction error as well as computational cost. In all cases, the acquired preprocessed dataset used to train logistic regression as my baseline model and the results will be compared in order to choose the most efficient preprocessing method. The selected method(s) then applied to the rest of the algorithms in the next steps.</p>
<h2 class="unnumbered" id="metrics">Metrics</h2>
<p>The kaggle leaderboard score is based on log loss (cross entropy) error. So, this evaluation metric will be used during this project.</p>
<p>Log-loss measures the accuracy of a classifier. It is used when the model outputs a probability for each class, rather than just the most likely class.</p>
<p>Indeed, Log-loss is a “soft” measurement of accuracy that incorporates the idea of probabilistic confidence. It is intimately tied to information theory: log-loss is the cross entropy between the distribution of the true labels and the predictions. Intuitively speaking, entropy measures the unpredictability of something. Cross entropy incorporate the entropy of the true distribution, plus the extra unpredictability when one assumes a different distribution than the true distribution.</p>
<h1 id="analysis">Analysis</h1>
<h2 class="unnumbered" id="data-exploration">Data Exploration</h2>
<p>In this project, I am dealing with a high dimensional binary classification problem in which each example is a molecule and the target is its correspondent biological activity. The features are the physical and chemical characteristics of each molecule using which I will predict the target. Some examples of molecular descriptors include: boiling point, melting point, heat capacity, Entropy, density, total surface area, molar volume <span class="citation" data-cites="leveleux2012"></span>. A summary of dataset description is presented as follows:</p>
<table>
<caption>Train &amp; Test set Size</caption>
<thead>
<tr class="header">
<th style="text-align: left;">Set</th>
<th style="text-align: left;">Rows</th>
<th style="text-align: left;">Columns</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Train</td>
<td style="text-align: left;">3752</td>
<td style="text-align: left;">1777</td>
</tr>
<tr class="even">
<td style="text-align: left;">Test</td>
<td style="text-align: left;">2502</td>
<td style="text-align: left;">1776</td>
</tr>
</tbody>
</table>
<table>
<caption>Independent Features</caption>
<thead>
<tr class="header">
<th style="text-align: left;">Index</th>
<th style="text-align: left;">Features</th>
<th style="text-align: left;">Type</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">2</td>
<td style="text-align: left;">D1</td>
<td style="text-align: left;">Numeric</td>
<td style="text-align: left;">Calculated properties</td>
</tr>
<tr class="even">
<td style="text-align: left;">3</td>
<td style="text-align: left;">D3</td>
<td style="text-align: left;">Numeric</td>
<td style="text-align: left;">Calculated properties</td>
</tr>
<tr class="odd">
<td style="text-align: left;">:</td>
<td style="text-align: left;">:</td>
<td style="text-align: left;">:</td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">1777</td>
<td style="text-align: left;">D1776</td>
<td style="text-align: left;">Numeric</td>
<td style="text-align: left;">Calculated properties</td>
</tr>
</tbody>
</table>
<table>
<caption>Dependent Features</caption>
<thead>
<tr class="header">
<th style="text-align: left;">Index</th>
<th style="text-align: left;">Features</th>
<th style="text-align: left;">Type</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">1</td>
<td style="text-align: left;">Activity</td>
<td style="text-align: left;">Cetegory</td>
<td style="text-align: left;">2 variables</td>
</tr>
</tbody>
</table>
<p><strong>Dataset Advantages:</strong></p>
<ul>
<li><p>There are no missing values.</p></li>
<li><p>It is normalized.</p></li>
<li><p>As it is shown in following Figure, the training set has balanced number of observations in each class. So, there is no need to deal with the problem of unbalanced labels.</p></li>
</ul>
<figure>
<img src="image01.jpg" id="figure1" alt="Number of observations in each class of training data." /><figcaption aria-hidden="true">Number of observations in each class of training data.</figcaption>
</figure>
<p><strong>Dataset Challenges:</strong></p>
<ul>
<li><p>This is a high dimensional sparse dataset. Using various feature reduction/selection methods I will try to eliminate the non-informative features and make a denser dataset in order to achieve a more precise and accurate prediction.</p></li>
<li><p>Since the acquired score in Kaggle should be reported as log loss error, I are only allowed to use probabilistic methods, which support cross entropy error function such as ensembles or neural network. (e.g. SVM is not applicable.)</p></li>
<li><p>Having an inhomogeneous feature space (both binary and numerical), neural network is not a promising method, because optimizing the hyperparameters would be extremely tricky. Although I are aware that this method might not be the best learner for this specific dataset, I still decided to try it as one of my preliminary models for comparison reasons.</p></li>
</ul>
<h2 class="unnumbered" id="algorithms-and-techniques">Algorithms and Techniques</h2>
<p>By randomly dividing the training data to 80% train set and 20% test set, I first train on the former and then predict on the later. Finally, the model will be fitted to the whole original train data and predicted on the final test set, the reported results are the scores from private leaderboard of Kaggle competition. The optimal hyper-parameters of all methods will be computed by a grid search over the wide range of possible values, using k-fold cross validation.</p>
<p>In order to tackle this binary classification task, I investigated the performance of various preprocessing and learning techniques. The classifiers include Logistic regression as the baseline method; some preliminary models consist of neural network and tree-based bagging and boosting, and finally an advanced ensemble named stacked generalization. Using real, high-dimensional molecules dataset, I demonstrated that stacked generalization method could reduce the cross entropy loss to the reliable value of 0.375.</p>
<p>The outline of tasks and algorithms which will be taken into account to tackle this problem is presented as below:</p>
<h3 class="unnumbered" id="preprocessing">Preprocessing</h3>
<ul>
<li><p><span>Dimensionality Reduction Using Principal Components Analysis (PCA)</span></p></li>
<li><p><span>Dimensionality Reduction Using Feature Ranking (Importance)</span></p></li>
<li><p><span>Outlier Detection Using One-class SVM</span></p></li>
<li><p>F<span>eature Weighted Linear Stacking (FWLS) </span></p></li>
</ul>
<h3 class="unnumbered" id="baseline-method">Baseline Method</h3>
<p>I apply first three preprocessing methods and also various combinations of them on baseline (Logistic Regression) in order to evaluate their performance. The last method, however, depends on stacking algorithm. More details will be provided in the methodology section.</p>
<h3 class="unnumbered" id="preliminary-method">Preliminary Method</h3>
<ul>
<li><p><span>Random Forests (RF)</span></p></li>
<li><p><span>Extremely Randomized Trees (ERT)</span></p></li>
<li><p><span>Adaboost</span></p></li>
<li><p><span>Gradient Boosting Classifier (GBC)</span></p></li>
<li><p><span>Neural Network (NN)</span></p></li>
</ul>
<h3 class="unnumbered" id="advanced-methods">Advanced Methods</h3>
<ul>
<li><p><span>Weighted majority Voting </span></p></li>
<li><p><span>Stacking (Stacked Generalization)</span></p></li>
</ul>
<h2 class="unnumbered" id="benchmark">Benchmark</h2>
<p>I choose Logistic regression (LR) as my baseline method. LR is considered as a generalized linear classification model that falls in the class of discriminative learning algorithms. The goal is to maximize the conditional likelihood (or equivalently to minimize the cross entropy error) of the output given the input data. It has a linear decision boundary.</p>
<p>LR will be implemented using scikit-learn library, in which I use L1 norm as the regularization in my cost function and liblinear as the optimization solver. The regularization hyper-parameter will be chosen based on a 3-fold cross validation.</p>
<h1 id="methodology">Methodology</h1>
<h2 class="unnumbered" id="data-preprocessing">Data Preprocessing</h2>
<p>Here, I present the methodology of all preprocessing methods except Feaure-Weighted Linear Stacking (FWLS). Since FWLS algorithm follows stacking concept, I explain it separately in implementation section after stacking method.<br />
</p>
<p>PCA was implemented as an unsupervised feature reduction method. PCA uses an orthogonal transformation for mapping the observations from original feature space to principal components space in which the variables are linearly uncorrelated. In this transformation, the first principal component is chosen to have the largest variance. Afterwards, the second principal component will be selected to have the highest possible variance considering orthogonality to the previous component and so on.</p>
<p>The optimal number of components could be achieved based on the explained variance of data while combined with a supervised method as described in scikit-learn documentation <span class="citation" data-cites="piplining"></span>. Here, I used logistic regression as a supervised method to measure PCA’s performance. As it is shown in following Figure, the first 240 principal components were sufficient to explain 95% variance of my data. PCA is sensitive to the relative scaling of the original variables; using already scaled dataset I did not encounter any problems.</p>
<figure>
<img src="figure_2.png" id="figure2" alt="Number of components based on explained variance." /><figcaption aria-hidden="true">Number of components based on explained variance.</figcaption>
</figure>
<p>Random Forests (RF) or Extremely Randomized Trees (ERT) can be used as a supervised method to rank the importance of features based on the technique provided by Leo Breiman <span class="citation" data-cites="breiman"></span>. While fitting an ERT/RF to data, the out of bag error (OOB) for each example is computed and averaged over all trees. After training, the values of each feature are permuted among the training data, afterwards, OOB error will be computed again on the perturbed data. The average on the difference between OOB error before and after permutation over all trees provides the importance score for each variable. The reported importance score is the normalized value using the standard deviation of these differences. Higher values are corresponding to higher ranks.</p>
<p>Using scikit-learn library, I ranked the features by ERT method <span class="citation" data-cites="featureimportance"></span>. As demonstrated in following Figure, all the importance scores associated with features are significantly small which suggests that practically almost all of the features are contributing to predict the target value. I chose the top 98% of the importance scores, which is equal to 430 top rank informative features.</p>
<figure>
<img src="figure_3.jpg" id="figure3" alt="Feature importance using extremely randomized trees." /><figcaption aria-hidden="true">Feature importance using extremely randomized trees.</figcaption>
</figure>
<p>The goal is to separate a colony of normal observations from some polluting examples, called “outliers”. One-class SVM is a promising unsupervised outlier/novelty detection method in high dimensions. Which, learns a decision function by estimating the support of a high-dimensional distribution and classifies new examples as similar or different to the training ones <span class="citation" data-cites="oneClass"></span>.</p>
<p>In this project, I implemented One-class SVM with non-linear kernel (RBF) using scikit-learn library, which resulted in a non-linear decision boundary. Following Figure shows how One-class SVM detects novelty/outlier examples. In this figure, taken from scikit-learn website <span class="citation" data-cites="oneClass"></span>, the decision boundary is shown with red circles and the new outliers are represented by red dots. In this case, 300 examples in the training set and 85 observations in the test set are detected as outliers.</p>
<figure>
<img src="plot_oneclass_001.png" id="figure4" alt="One-class SVM with RBF kernel for outlier/novelty detection." /><figcaption aria-hidden="true">One-class SVM with RBF kernel for outlier/novelty detection.</figcaption>
</figure>
<h2 class="unnumbered" id="implementation">Implementation</h2>
<p>Having inhomogeneous features on one hand, and being limited to cross entropy error function on the other hand, I chose tree-based ensemble methods as my preliminary models. As described before, although neural network might not be the best option for inhomogeneous datasets, I decided to try it for comparison reasons. The preliminary methods are as follows: Random Forests (RF), Extremely Randomized Trees (ERT), Adaboost, Gradient Boosting Classifier (GBC), and Neural Network (NN).</p>
<p>Bagging (bootstrap aggregating) is an ensemble averaging meta-algorithm that improves the prediction accuracy by reducing the variance. In decision tree based bagging, each bootstrapped samples is used as a train set to grow a decision tree and the result is the average over probabilistic predictions of all trees. Random forests uses the idea of bagging combined with random subspace selection of features at each step of splitting trees in order to more control the variance with a slight increase in bias. The most advanced ensemble averaging method is extremely randomized trees, in which randomness goes one step further and even the splitting rule at each node is selected randomly. This results in higher gain considering variance reduction goal.</p>
<p>In this project, RF is implemented using scikit-learn library. I used information gain as the function to measure the quality of splits, with the optimal number of 400 trees, and the squared of total number of features as the subspace features selection. I did not apply any limit on the depth of trees and splitting continued until there was at least one sample at each leaf. The number of estimators (trees) were selected via a grid search over wide range of possible values by applying a 3 fold cross validation. The exact same criteria applied to ERT.</p>
<p>Boosting methods were the other group of ensemble models considered at this step. These methods are built sequentially over weak classifiers in order to reduce the bias. The final meta-algorithm is a linear weighted combination of the base estimators with a reduced generalization error. One of the most well known boosting models is Adaptive boosting (Adaboost). At each iteration of boosting algorithm, the training samples that were incorrectly predicted at previous step are given more weights so the classifier would focus more on the misclassified observations next time. In Adaboost, the automatic reweighting is equivalent to recalculating of the error at each stage. Another common boosting technique is gradient boosting classifier, which is a generalization of boosting to arbitrary differentiable loss functions using decision trees as base estimators. Overall, the boosting classifiers are advantages while working with heterogeneous features.</p>
<p>The contribution of each classifier (tree) in boosting algorithm shrinks by the value of learning rate, so it becomes as a hyperparameter for my method besides the number of trees. In this project, I used scikit-learn library to implement Adaboost algorithm. The decision tree is used as the base estimator. I did a grid search over various combinations of number of trees and learning rate. The optimum 300 numbers of trees were chosen, however, in the case of perfect fit, the learning procedure would be stopped early. The optimal achieved learning rate was 0.001.</p>
<p>For implementation of GBC I also used scikit-learn library. I chose half of the samples to be used for fitting the individual base learner because of computational reasons. This results in a reduction of variance and a slight increase in bias; however, the boosting algorithm is designed to handle the high bias problem. The optimal value of the hyperparameters achieved by a grid search and 5-fold cross validation are as follows: learning rate = 0.05, maximum depth of trees = 6, and the number of estimators = 300.</p>
<p>The final implemented method is Neural Network (NN). NN is an information-processing paradigm, which is inspired by the information processing structure of biological nervous systems. A NN consists of a large number of interconnected processing units, named neurons, working in unison to solve a particular problem. Through a learning process, the network is trained for a specific application such as classification. Some NNs advantages are as follows: remarkable ability to derive meaning from complicated data, adaptive learning, self-organization, and real time operations <span class="citation" data-cites="aleksander1990"></span>.</p>
<p>In this project, a single layer feed forward NN was implemented using theano <span class="citation" data-cites="yadav2015"></span>. Weights were trained by backpropagation algorithm and the loss function was the cross entropy loss. The acquired hyperparameters are as follows: learning rate= 0.01, number of epochs = 200, number of hidden layers= 25, L2 regularization parameter = 0.001, and batch size = 200.</p>
<h2 class="unnumbered" id="stacked-generelization-stacking">Stacked Generelization (Stacking)</h2>
<p>In order to improve my results, I tried a more sophisticated method at this step. I started with some classical advanced methods such as hard and soft weighted majority voting, however, none of them provided better results than preliminary methods. I then moved to stacked generalization (stacking) method, which is a meta-learner consists of multiple model mixtures. The idea is to learn a function that combines the predictions of the individual classifiers and feed them as an input to the final meta classifier. This method was originally introduced on 1992 by David H. Wolpert <span class="citation" data-cites="wolpert1992"></span>, however, I got the core idea of using stacking for this specific competition from a script written by Emanuele Olivetti <span class="citation" data-cites="emanuel"></span>, who achieved 17th rank on Kaggle. I made significant modifications on the original code and improved it to the rank 6th. The algorithm’s steps are as follows:</p>
<div class="algorithm">
<div class="algorithmic">
<p>Train <span class="math inline"><em>n</em></span> different classifiers <span class="math inline"><em>C</em><sub>1</sub>, ⋯, <em>C</em><sub><em>n</em></sub></span> (the baseline classifiers) Obtain predictions of the classifiers for the training examples. Form a new data set: the meta data. Train a separate classifier <span class="math inline"><em>M</em></span> as the meta classifier.</p>
</div>
</div>
<p>Each base classifier’s prediction is used as a feature in the meta dataset ( e.g. data fed to the second stage meta classifier), besides, in this algorithm, the classes are kept same as the original dataset. Although the steps 1-3 of the stacking algorithm are similar to cross validation approach, here, I combine the baseline classifiers non-linearly instead of using a winner-takes-all approach <span class="citation" data-cites="wolpert1992"></span>. Regarding practical applications, this method demonstrated outstanding results in the Netflix prize competition <span class="citation" data-cites="jahrer"></span>.</p>
<p>A concept diagram of stacking mechanism is demonstrated in following Figure.</p>
<figure>
<img src="figure5.png" id="figure5" alt="Concept diagram of stacking." /><figcaption aria-hidden="true">Concept diagram of stacking.</figcaption>
</figure>
<p>Scikit-learn library is used for the implementation of all algorithms included in stacking method. For all of the ensembles, decision trees chosen as their base learners.</p>
<h2 class="unnumbered" id="feature-weighted-linear-stacking-fwls">Feature-Weighted Linear Stacking (FWLS)</h2>
<p>FWLS is a preprocessing method, however, because of its dependency on stacking concept, I provide its methodology here.</p>
<p>I apply a feature-weighted linear stacking model to select the most important features as a preprocessing method and then use the stacking approach for classification. Suppose we have m distinct predictors obtained from a given training set. For example, the first predictor could be a logistic regression model, a second one a naïve bayes and a third one a random forest. The idea of the FWLS model is to train different base classifiers on the training dataset. The outputs of all baseline classifiers are probabilities and class predictions. I use Ridge regression approach for modelling the relationship between a prediction <span class="math inline"><em>y</em><sub><em>m</em> × 1</sub></span> obtained from each baseline classifier and the independent variable <span class="math inline"><em>X</em><sub><em>n</em> × <em>m</em></sub></span> as meta-features: a form of FWLS <span class="citation" data-cites="sill2009"></span>. Afterwards, I stack meta-features from different classifiers together using a logistic regression. The hope is that the stacking model learns which base model is the best predictor for samples with a certain feature value <span class="math inline">(<em>w</em><sub><em>m</em> × 1</sub>)</span>. A diagram of stacking meta-features is demonstrated in following Figure.</p>
<figure>
<img src="fwls.png" id="figF1" alt="Concept Diagram of Feature-Weighted Linear Stacking" /><figcaption aria-hidden="true">Concept Diagram of Feature-Weighted Linear Stacking</figcaption>
</figure>
<p>By applying this method in preprocessing step, I can predict the linear regression of probability of meta-features and select the <span class="math inline"><em>k</em></span> most probable ones. For selecting the best number of reduced features, <span class="math inline"><em>k</em></span>, we run several experiments by 5-fold cross validation and choose <span class="math inline"><em>k</em> = 440</span> as the best value for reduced features. Furthermore, finding optimal values for penalty parameters of the Ridge regression, <span class="math inline"><em>α</em></span>, can be quite tedious. several search processes were run to check the best value for <span class="math inline"><em>α</em></span>. Results given by a 5-fold cross validation for different values of the <span class="math inline"><em>α</em></span> indicates that <span class="math inline"><em>α</em> = .31622</span> is the best choice. Following Figure presents regularization parameters optimization plot.</p>
<figure>
<img src="ridgeCV.png" id="fig:2F" alt="Regularization parameter optimization plot - optimal \alpha = 0.31622. " /><figcaption aria-hidden="true">Regularization parameter optimization plot - optimal <span class="math inline"><em>α</em> = 0.31622</span>. </figcaption>
</figure>
<h2 class="unnumbered" id="refin">Refinement</h2>
<p>I did a grid search and 10-fold cross validation for hyperparameter tuning. I also tried various combinations of classifiers. Following Table shows some of results that I obtained during the refinement process. I have also tested double stacking. It means that for the second stage, I implemented other stacking algorithm. The results are presented in the following Table.</p>
<table>
<caption>Various combinations of classifiers and results improvement</caption>
<thead>
<tr class="header">
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Level 0</th>
<th style="text-align: center;">Level 1</th>
<th style="text-align: center;">LogLoss Error</th>
<th style="text-align: center;">Kaggle Rank</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;" rowspan="2">CLF 1</td>
<td style="text-align: left;">RF(gini)+RF(E)+ET(gini)+</td>
<td style="text-align: left;" rowspan="2">Adaboost</td>
<td style="text-align: center;" rowspan="2">0.601</td>
<td style="text-align: center;" rowspan="2">629</td>
</tr>
<tr class="even">
<td style="text-align: left;">ET(entropy)+GBM+AdaBoost</td>
</tr>
<tr class="odd">
<td style="text-align: center;" rowspan="2">CLF 2</td>
<td style="text-align: left;">RF(gini)+RF(entropy)+ET(gini)+</td>
<td style="text-align: left;" rowspan="2">ET(entropy)</td>
<td style="text-align: center;" rowspan="2">0.982</td>
<td style="text-align: center;" rowspan="2">670</td>
</tr>
<tr class="even">
<td style="text-align: left;">ET(entropy)+GBM+AdaBoost</td>
</tr>
<tr class="odd">
<td style="text-align: center;">CLF 3</td>
<td style="text-align: left;">RF(gini)+ET(gini)+GBM</td>
<td style="text-align: left;">LR+NB+RF(gini)+Voting</td>
<td style="text-align: center;">0.507</td>
<td style="text-align: center;">264</td>
</tr>
<tr class="even">
<td style="text-align: center;">CLF 4</td>
<td style="text-align: left;">RF(gini)+ET(gini)+GBM</td>
<td style="text-align: left;">GBM</td>
<td style="text-align: center;">0.478</td>
<td style="text-align: center;">258</td>
</tr>
<tr class="odd">
<td style="text-align: center;" rowspan="2">CLF 5</td>
<td style="text-align: left;">RF(gini)+RF(entropy)+ET(gini)+</td>
<td style="text-align: left;" rowspan="2">GBM</td>
<td style="text-align: center;" rowspan="2">0.516</td>
<td style="text-align: center;" rowspan="2">116</td>
</tr>
<tr class="even">
<td style="text-align: left;">ET(entropy)+GBM</td>
</tr>
<tr class="odd">
<td style="text-align: center;" rowspan="2">CLF 6</td>
<td style="text-align: left;">RF(gini)+RF(entropy)+ET(gini)+</td>
<td style="text-align: center;" rowspan="2">LR+NB+RF(gini)+Voting</td>
<td style="text-align: center;" rowspan="2">0.484</td>
<td style="text-align: center;" rowspan="2">115</td>
</tr>
<tr class="even">
<td style="text-align: left;">ET(entropy)+GBM</td>
</tr>
<tr class="odd">
<td style="text-align: center;">CLF 7</td>
<td style="text-align: left;">LR+NB+RF(gini)</td>
<td style="text-align: left;">LR+NB+RF(gini)+Voting</td>
<td style="text-align: center;">0.489</td>
<td style="text-align: center;">133</td>
</tr>
<tr class="even">
<td style="text-align: center;">CLF 8</td>
<td style="text-align: left;">RF(gini)+ET(gini)+GBM</td>
<td style="text-align: left;">LR</td>
<td style="text-align: center;">0.453</td>
<td style="text-align: center;">70</td>
</tr>
<tr class="odd">
<td style="text-align: center;" rowspan="2">CLF 9</td>
<td style="text-align: left;">RF(gini)+RF(entropy)+ET(gini)+</td>
<td style="text-align: left;" rowspan="2">LR</td>
<td style="text-align: center;" rowspan="2">0.447</td>
<td style="text-align: center;" rowspan="2">17</td>
</tr>
<tr class="even">
<td style="text-align: left;">ET(entropy)+GBM</td>
</tr>
<tr class="odd">
<td style="text-align: center;">CLF 10</td>
<td style="text-align: left;">LR+NB+RF(gini)</td>
<td style="text-align: left;">LR</td>
<td style="text-align: center;">0.455</td>
<td style="text-align: center;">19</td>
</tr>
</tbody>
</table>
<p>At the end, my best method includes logistic regression as meta classifier (level 1) and the models provided in following Table as level 0 learners (with their attributes/optimal hyperparameters indicated in front of them). Considering a trade–off between computational cost and the prediction accuracy I fixed the number of trees for all of my level 0 classifiers to 100. More detailed results are provided in the results section.</p>
<p>By applying this setting I obtain the logloss error of <strong>0.375</strong> and the rank of <strong>6</strong> in the leaderboard.</p>
<table>
<caption>The base classifiers used in level 0 of my best stacking model</caption>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Base Classifier</strong></th>
<th style="text-align: left;"><strong>Attributes/Hyperparameters</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">RandomForests</td>
<td style="text-align: left;">criterion= gini (Gini impurity)</td>
</tr>
<tr class="even">
<td style="text-align: left;">RandomForests</td>
<td style="text-align: left;">criterion=entropy (information gain)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">ExtremelyRandomizedTrees</td>
<td style="text-align: left;">criterion=gini (Gini impurity)</td>
</tr>
<tr class="even">
<td style="text-align: left;">ExtremelyRandomizedTrees</td>
<td style="text-align: left;">criterion=entropy (information gain)</td>
</tr>
<tr class="odd">
<td style="text-align: left;" rowspan="2">GradientBoostingClassifier</td>
<td style="text-align: left;">Learning rate=0.05, subsample=0.5,</td>
</tr>
<tr class="even">
<td style="text-align: left;">maximum depth=6</td>
</tr>
<tr class="odd">
<td style="text-align: left;">AdaBoost</td>
<td style="text-align: left;">Learning rate=0.05</td>
</tr>
</tbody>
</table>
<h1 id="results">Results</h1>
<h2 class="unnumbered" id="baseline-method-prgprocessing">Baseline Method &amp; Prgprocessing</h2>
<p>In following Figure, the results of applying various preprocessing methods on logistic regression (baseline classifier) are presented.</p>
<figure>
<img src="logloss.jpg" id="figure6" alt="Results of baseline method (LR) with and without preprocessing." /><figcaption aria-hidden="true">Results of baseline method (LR) with and without preprocessing.</figcaption>
</figure>
<p>The preprocessing methods do not show any substantial impact on my results, which implies this dataset does not have significant number of outliers or irrelevant features. Since using PCA was slightly improved the result of the base logistic regression, I decided to try all my preliminary models both with and without PCA, at the next step.</p>
<p>Following Figure shows the ROC curve of logistic regression classifier, which visually displays how far I am from an accurate classification (a perfect classification is achieved when ideally the curve reaches the top left corner of the graph):</p>
<figure>
<img src="roc.png" id="figure7" alt="ROC curve of logistic regression." /><figcaption aria-hidden="true">ROC curve of logistic regression.</figcaption>
</figure>
<h2 class="unnumbered" id="preliminiary-methods">Preliminiary Methods</h2>
<p>Following Figure summarizes the results for all preliminary methods with and without applying PCA.</p>
<figure>
<img src="image08.jpg" id="figure8" alt="Results of all preliminary methods with and without PCA." /><figcaption aria-hidden="true">Results of all preliminary methods with and without PCA.</figcaption>
</figure>
<p>Although none of the preliminary methods led us to the top of the leaderboard, they all improved the baseline. Moreover, using PCA as a dimensionality reduction technique on all my preliminary methods always resulted in larger errors, so I decided not to apply it on my advanced methods, at the upcoming steps.</p>
<p>Altogether, comparing the results of all methods without PCA, it is evident that they are just slightly different from each other. As expected and explained before, most of the ensemble methods outperformed NN. The best result is given by gradient boosting classifier with 0.405 cross entropy loss, which is not significantly larger than random forests with 0.423 error. Adaboost with a negligible difference performed somewhat worse than NN. Below, Following Figure is the learning curve of the best-performed preliminary method (gradient boosting classifier):</p>
<figure>
<img src="figure9.png" id="figure9" alt="Learning curve of gradient boosting classifier." /><figcaption aria-hidden="true">Learning curve of gradient boosting classifier.</figcaption>
</figure>
<p>The score on the learning curve plot represents accuracy. Since the validation score is much less than the training score for the maximum number of available training examples, I probably could have reached higher accuracy if I had more observations. Moreover, I need a more flexible estimator to more accurately fit the model to this somehow complex dataset.</p>
<h2 class="unnumbered" id="stacking-and-fwls"> Stacking and FWLS</h2>
<p>I achieved the best result using stacked generalization with cross entropy error of 0.375. This method outperformed the previous ones as well as hard and soft majority voting, because of its capability to reduce bias and handle overfitting. Here, I only explain and analyze the result of stacking model as my best method in order to respect the required length limit of the report. However, you can find all codes with comments and explanations, including majority voting, in an iPython notebook named Codes_capstoneProject.ipynb.</p>
<p>Stacking meta-learner represents a single hypothesis, however, it is not necessarily contained within the hypothesis space of the models from which it is built. Hence, stacking can be indicated to have more flexibility in the functions that it can represent. This flexibility can, in theory, enable them to overfit the training data more than a single model would, but in practice, these techniques tend to reduce the problems related to overfitting. Stacking reduces variance due to the fact that the results are less dependent on the characteristics of a single training set <span class="citation" data-cites="Boris"></span>.</p>
<p>Additionally, it reduces bias since it is a combination of multiple classifiers that learns a more sophisticated concept class compared to a single base classifier. In other words, if a baseline classifier incorrectly learns a certain hypothesis, the second layer (meta) classifier is able to detect this undesired learning.</p>
<p>My experiments also show that linear stacking (i.e. using linear meta classifier in level 1 such as logistic regression or perceptron) considerably outperforms any other stacking in which more sophisticated algorithms were involved in meta layer. This probably happens because of overfitting, which is resulted from using a complex classifier.</p>
<p>Below, Tables are the confusion matrix and the classification report of my best stacking method, respectively. As described before, I randomly divided the dataset to 80% train and 20% test set, trained on the former and predicted on the later. Therefore, the analyses presented here are based on 751 test observations.</p>
<table>
<caption>Confusion Matrix of Stacking Method</caption>
<thead>
<tr class="header">
<th style="text-align: center;">N = 751</th>
<th style="text-align: center;">Predicted Class 0</th>
<th style="text-align: center;">Predicted Class 1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Actual Class 0</td>
<td style="text-align: center;">TN = 252</td>
<td style="text-align: center;">FP = 76</td>
</tr>
<tr class="even">
<td style="text-align: center;">Actual Class 1</td>
<td style="text-align: center;">FN = 74</td>
<td style="text-align: center;">TP = 349</td>
</tr>
</tbody>
</table>
<table>
<caption>Classification Report of Stacking Method </caption>
<thead>
<tr class="header">
<th style="text-align: center;">Class</th>
<th style="text-align: center;">Precision</th>
<th style="text-align: center;">Recall</th>
<th style="text-align: center;">F1-score</th>
<th style="text-align: center;">Support</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">328</td>
</tr>
<tr class="even">
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.82</td>
<td style="text-align: center;">0.83</td>
<td style="text-align: center;">0.82</td>
<td style="text-align: center;">423</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Avg/Total</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">751</td>
</tr>
</tbody>
</table>
<p>According to above Tabels, the number of misclassified observations is almost the same in both classes. Moreover, the recall and precision within each class are the same, which implies the same concept. So, there is no tendency towards a specific label which was expected because of balanced number of examples in each class. Besides, Class one has somewhat larger f1-score, meaning that the test prediction for this class is slightly more accurate.</p>
<p>As a final technique, I applied FWLS on stacking. As for any other complex method, I tried various configurations both for FWLS and stacking. Among all combinations, stacking with logistic regression, naïve bayes, random forests and FWLS with random forest, extremely randomized trees, and gradient boosting classifier resulted in the lowest error and highest rank of 0.379 and 27, respectively. In general, considering all my experiments, using FWLS did not exceed the performance of best stacking method in any cases.</p>
<p>Despite a slight increase in log loss error while using FWLS, we can benefit from the reduced feature space by decreasing the number of features. This provides higher computational efficiency in the case that the prediction accuracy is not my first priority. The results of using FWLS method approve my previous conclusion regarding affectlessness of feature reduction techniques on this dataset.</p>
<h1 id="conclusion">Conclusion</h1>
<h2 class="unnumbered" id="free-form-visualization">Free-Form Visualization</h2>
<p>In the free form visualization, I am focusing on the evaluation of the rank and logloss error by implementing different approaches (see following Figure ). The name of classifiers in the horizontal axes are given in the algorithm refinement Section.</p>
<figure>
<img src="evolution.png" id="figureevo" alt="Evolution of Kaggle Rank and LogLoss Error." /><figcaption aria-hidden="true">Evolution of Kaggle Rank and LogLoss Error.</figcaption>
</figure>
<h2 class="unnumbered" id="reflection">Reflection</h2>
<p>This report presents a comprehensive evaluation of a binary classification problem of predicting a biological response of a molecule based on its characteristics. An accurate prediction of this task is of great importance in pharmaceutical industry since it can save a huge amount of avoidable cost and repetitive work. In order to increase the prediction accuracy, I investigated various machine learning techniques including different learning and preprocessing methods.</p>
<p>According to the results of all applied preprocessing methods, this dataset performed better without any kind of feature reduction/selection or outlier detection. It can be concluded that a great fraction of features are correlated with the target and dealing with a somehow clean dataset without lots of outliers.</p>
<p>I got the benchmark error score using logistic regression as the baseline. Although it was not extremely high, there was a possibility of reducing it by using more suitable and complex methods. The higher error resulted from a linear classifier implies that the classes are not linearly separable.</p>
<p>On one hand, being limited to the log loss error function, and on the other hand, having inhomogeneous features, I selected bagging and boosting as the next level more complex and non-linear methods. I also tried neural network and as expected and explained before, did not get my best result out of it. All of the ensemble models outperformed the baseline and among all, gradient boosting classifier provided the lowest error with only a slight difference from the others. Proving the fact that tree-based ensemble methods are a great choice for this specific task.</p>
<p>I reduced the error from 0.481 for logistic regression (with PCA) to an acceptable level of 0.405 using Gradient boosting classifier, however, stacking helped us to further decrease it to 0.375, which means a huge jump from rank 254 to 6 on the Kaggle private leaderboard. As described in great details before, the reason behind high performance of stacking is its capability of handling both overfitting and high bias problems.</p>
<h2 class="unnumbered" id="improvement">Improvement</h2>
<p>One of the great improvement to be considered as a future work for this problem could be doing more deep feature engineering. Since, regarding to the given data set we do not have any information about the nature of features, understanding features can lead to better feature engineering and possible error reduction.</p>
<p>Another further work for this project can be investigating more number of possible combinations of classifiers. This can help to obtain better results, since the winners of the competition have used much more number of models.</p>
<div class="thebibliography">
<p><span>99</span></p>
<p>Leveleux, Thomas, Eliana Penzner, and Chris Riley. "Predicting A Biological Response Rate - YWF Consulting." Wikispaces, 4 May 2012. Web. 13 Dec. 2015. <a href="&lt;https://gsm672.wikispaces.com/Predicting A Biological Response Rate - YWF Consulting&gt;">&lt;https://gsm672.wikispaces.com/Predicting A Biological Response Rate - YWF Consulting&gt;</a>.</p>
<p>Jain, Ajay N., Kimberle Koile, and David Chapman. "Compass: predicting biological activities from molecular surface properties. Performance comparisons on a steroid benchmark." Journal of Medicinal Chemistry 37.15 (1994): 2315-2327.</p>
<p>"Biological activity." Wikipedia: The Free Encyclopedia. Wikimedia Foundation, Inc. 15 July 2014. Web. 12 Dec. 2015.<a href=" &lt;https://en.wikipedia.org/wiki/Biological_activity&gt;"> &lt;https://en.wikipedia.org/wiki/Biological_activity&gt;</a></p>
<p>Dua, Sumeet, U. Rajendra Acharya, and Prerna Dua. Machine learning in healthcare informatics. Springer Berlin Heidelberg, 2014.</p>
<p>"Pipelining: Chaining a PCA and a Logistic Regression." Pipelining: Chaining a PCA and a Logistic Regression — Scikit-learn 0.17 Documentation. Web. 12 Dec. 2015. <a href="&lt;http://scikit-learn.org/stable/auto_examples/plot_digits_pipe.html&gt;">&lt;http://scikit-learn.org/stable/auto_examples/plot_digits_pipe.html&gt;</a>.</p>
<p>Breiman, Leo. "Random forests." Machine learning 45.1 (2001): 5-32.</p>
<p>"Feature Importances with Forests of Trees." Feature Importances with Forests of Trees — Scikit-learn 0.17 Documentation. Web. 12 Dec. 2015. <a href="&lt;http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html&gt;">&lt;http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html&gt;</a>.</p>
<p>"One-class SVM with Non-linear Kernel (RBF)." One-class SVM with Non-linear Kernel (RBF) — Scikit-learn 0.17 Documentation. Web. 12 Dec. 2015. <a href="&lt;http://scikit-learn.org/stable/auto_examples/svm/plot_oneclass.html&gt;">&lt;http://scikit-learn.org/stable/auto_examples/svm/plot_oneclass.html&gt;</a>.</p>
<p>Aleksander, Igor, and Helen Morton. An introduction to neural computing. Vol. 3. London: Chapman &amp; Hall, 1990.</p>
<p>Yadav, Lavish. "Lavishyadav/Predicting-Biological-Response." GitHub. Web. 12 Dec. 2015.<a href="&lt;https://github.com/lavishyadav/Predicting-Biological-Response/blob/master/Single layer neural network.py&gt;">&lt;https://github.com/lavishyadav/Predicting-Biological-Response/blob/master/Single layer neural network.py&gt;</a>.</p>
<p>Wolpert, David H. "Stacked generalization." Neural networks 5.2 (1992): 241-259.</p>
<p>Olivetti, Emanuele. "Pribadihcr/kaggle-bioresponse." GitHub. Web. 12 Dec. 2015. <a href="&lt;https://github.com/emanuele/kaggle_pbr/blob/master/blend.py&gt;">&lt;https://github.com/emanuele/kaggle_pbr/blob/master/blend.py&gt;</a>.</p>
<p>Jahrer, Michael, Andreas Töscher, and Robert Legenstein. "Combining predictions for accurate recommender systems." Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2010.</p>
<p>Aleskerov, Fuad, Boris Goldengorin, and Panos M. Pardalos. Clusters, Orders, and Trees: Methods and Applications In Honor of Boris Mirkin’s 70th Birthday. Springer Publishing Company, Incorporated, 2014.</p>
<p>Sill, Joseph, et al. "Feature-weighted linear stacking." arXiv preprint arXiv:0911.0460 (2009).</p>
</div>
</body>
</html>
